# ğŸ³ Offline Vision + Voice Chef Assistant

A privacy-preserving, offline cooking assistant designed for visually impaired users. Runs entirely on Raspberry Pi (or Windows) with **no internet connection** required during operation. Features **continuous voice listening** for truly hands-free cooking.

## ğŸ¯ Key Features

- **ğŸ¤ Continuous Voice Mode** - Hands-free operation with automatic voice detection (NEW!)
- **ğŸ‘ï¸ Live Ingredient Recognition** - Identify spices and ingredients with spatial guidance
- **ğŸ“ Quantity Detection** - Detect teaspoons, tablespoons, cups, grams using visual analysis
- **âœ… Recipe Adherence** - Track what's added, compare against recipe, suggest corrections
- **ğŸ”Š Voice-First UX** - Offline speech-to-text and text-to-speech
- **âš ï¸ Safety Warnings** - Alerts for hot surfaces, knives, steam, and dangerous steps
- **ğŸ”’ Strictly Offline** - All models and data on device, zero telemetry
- **ğŸ–¥ï¸ Cross-Platform** - Works on Raspberry Pi OS, Linux, and Windows

## ğŸŒŸ What Makes This Special

### Truly Hands-Free Cooking
Unlike traditional voice assistants that require wake words or button presses, our **Continuous Voice Loop** is always listening. Just speak naturally:

```
YOU: "Next step"
ASSISTANT: "Step 2 of 8. Heat 2 teaspoons oil. Caution: hot oil."

YOU: "What is this?"
ASSISTANT: "Hold the item steady... This looks like turmeric, about half a teaspoon."

YOU: "How much do I need?"
ASSISTANT: "Recipe needs a quarter teaspoon. Shall I guide you to remove some?"
```

No wake words. No buttons. Just cooking.

### Privacy-First Design
- âœ… **100% offline** - No internet required during cooking
- âœ… **Zero telemetry** - No data collection or tracking
- âœ… **Local processing** - All AI runs on your device
- âœ… **Temporary audio** - Voice data never saved permanently

### Accessible by Design
- â™¿ **Built for visually impaired users** - Voice-first interface
- ğŸ—£ï¸ **Natural language** - Warm, calm, practical instructions
- ğŸ” **Repetition on demand** - "Repeat" command anytime
- ğŸ“ **Spatial guidance** - "Left of stove", "front right"
- âš ï¸ **Safety confirmations** - Extra warnings for dangerous steps

---

## ğŸ“‹ Hardware Requirements

### Raspberry Pi (Recommended for Production)
- **Board**: Raspberry Pi 4/5 with 8GB RAM
- **Storage**: 128GB+ SD card
- **Camera**: USB webcam (640x480 minimum)
- **Microphone**: USB or 3.5mm omnidirectional mic
- **Audio Output**: HDMI, 3.5mm, or USB speakers
- **Optional**: Cooling fan

### Windows PC (Development/Testing)
- **RAM**: 8GB minimum, 16GB recommended
- **Storage**: 10GB free space
- **Camera**: Built-in or USB webcam
- **Microphone**: Built-in or external
- **Audio**: Any speakers or headphones

---

## ğŸš€ Quick Start

### Step 1: Installation

#### Raspberry Pi OS / Linux
```bash
chmod +x install_offline_linux.sh
./install_offline_linux.sh
```

#### Windows
```cmd
install_offline_windows.bat
```

### Step 2: Download Models

```bash
python scripts/download_models.py
```

**Manual Vision Model Download:**
- Visit: https://huggingface.co/vikhyatk/moondream2
- Download: `moondream2-text-model-f16.gguf` (or Q4 quantized)
- Save to: `models/vision/moondream2-q4.gguf`

### Step 3: Run

#### ğŸ¤ Voice Mode (Hands-Free)

```bash
# Linux/Raspberry Pi
./run_chef.sh --voice --recipe recipes/poha.json

# Windows
run_chef.bat --voice --recipe recipes\poha.json
```

#### âŒ¨ï¸ Interactive Mode (Testing)

```bash
./run_chef.sh --interactive
```

---

## ğŸ¤ Voice Commands

Just speak naturally:

- **"Next step"** - Proceed to next instruction
- **"What is this?"** - Identify ingredient
- **"How much?"** - Check quantity
- **"Repeat"** - Hear last instruction again
- **"Help"** - List available commands
- **"Stop"** - End cooking session

---

## ğŸ“– Usage Example

```
ASSISTANT: "Hello! Let's cook Poha together. Say 'next step' when ready."

YOU: "Next step"
ASSISTANT: "Step 1 of 8. Rinse 2 cups poha and drain."

YOU: "What is this?"
ASSISTANT: "Hold the item steady... This looks like poha."

YOU: "How much?"
ASSISTANT: "I see approximately 2 cups. That matches the recipe perfectly!"
```

---

## ğŸ”§ Configuration

Edit `config/default_config.yaml`:

```yaml
vision_model: ./models/vision/moondream2-q4.gguf
whisper_model: ./models/stt/ggml-base.en.bin
piper_voice: ./models/tts/en_US-amy-low.onnx
ocr_languages: eng+deva
sample_rate: 16000
vad_threshold: 0.02
```

---

## ğŸ› ï¸ Troubleshooting

### Voice Not Detected

```bash
# Calibrate VAD when starting voice mode
./run_chef.sh --voice
# Say 'y' when asked to calibrate

# Test microphone
arecord -d 5 test.wav && aplay test.wav  # Linux
```

### PyAudio Not Installed

```bash
# Linux/Raspberry Pi
sudo apt-get install portaudio19-dev python3-dev
pip install pyaudio
```

### Performance Issues (Raspberry Pi)

- Use smaller models (ggml-tiny.en.bin for STT)
- Enable 4GB swap file
- Reduce camera resolution

See [VOICE_MODE_GUIDE.md](VOICE_MODE_GUIDE.md) for detailed troubleshooting.

---

## ğŸ“š Documentation

- **[VOICE_MODE_GUIDE.md](VOICE_MODE_GUIDE.md)** - Complete voice mode documentation
- **[QUICKSTART.md](QUICKSTART.md)** - 5-minute setup guide
- **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** - Technical overview
- **[REQUIREMENTS_AUDIT.md](REQUIREMENTS_AUDIT.md)** - Compliance status
- **[CHANGELOG.md](CHANGELOG.md)** - Version history

---

## ğŸ“ Project Structure

```
chef-assistant/
â”œâ”€â”€ chef_assistant.py          # Main orchestrator with voice loop
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ quantity_estimator.py  # Quantity detection
â”‚   â”œâ”€â”€ recipe_validator.py    # Recipe adherence
â”‚   â”œâ”€â”€ vision_vlm.py          # Vision LLM wrapper
â”‚   â”œâ”€â”€ stt_whisper.py         # Speech-to-text with continuous listening
â”‚   â”œâ”€â”€ tts_piper.py           # Text-to-speech
â”‚   â””â”€â”€ ocr_tesseract.py       # OCR for labels
â”œâ”€â”€ recipes/
â”‚   â”œâ”€â”€ poha.json              # Sample: Poha
â”‚   â””â”€â”€ dal_tadka.json         # Sample: Dal Tadka
â”œâ”€â”€ knowledge/
â”‚   â””â”€â”€ spices.yaml            # Spice knowledge base
â”œâ”€â”€ models/                    # Model files (download separately)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.py     # Model downloader
â”‚   â””â”€â”€ calibrate.py           # Calibration tool
â””â”€â”€ tests/                     # Test suite (35+ tests)
```

---

## ğŸ”’ Privacy & Offline Operation

- **No internet required** during cooking
- **No telemetry** or data collection
- All processing happens **locally**
- **Voice data never saved** permanently
- Logs stored only on local filesystem

---

## âš ï¸ Safety Features

- Warns about hot surfaces, knives, steam
- Confirms dangerous actions before proceeding
- Allows pause and repetition of steps
- Uses clear spatial language

---

## ğŸ“ Calibration (Optional)

```bash
python scripts/calibrate.py
```

Calibrates:
1. Camera scale (pixels to cm)
2. Measuring spoon dimensions
3. Voice Activity Detection threshold

---

## ğŸ§ª Testing

```bash
pytest tests/ -v
```

---

## ğŸŒ Localization

Currently supports:
- **English** (primary)
- **Hindi/Marathi** (OCR for Devanagari labels)

---

## ğŸ“š Adding Custom Recipes

Create JSON file in `recipes/` directory:

```json
{
  "name": "Your Recipe",
  "serves": 2,
  "ingredients": [
    {
      "ingredient": "turmeric",
      "amount": 0.5,
      "unit": "teaspoon"
    }
  ],
  "steps": [
    {
      "instruction": "Add turmeric and mix",
      "safety": ["Caution: hot pan"],
      "check": {
        "ingredient": "turmeric",
        "amount": 0.5,
        "unit": "teaspoon"
      }
    }
  ]
}
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Wake word detection
- Multi-language support
- More recipes
- Custom spoon detector
- Performance optimizations

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with:
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Vision LLM inference
- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) - Speech recognition
- [Piper](https://github.com/rhasspy/piper) - Text-to-speech
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) - OCR
- [Moondream2](https://huggingface.co/vikhyatk/moondream2) - Vision model
- [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/) - Audio I/O

---

## ğŸ†• What's New - Version 1.1

### Continuous Voice Loop (NEW!)
- âœ… Hands-free operation with automatic voice detection
- âœ… Always-listening mode with VAD
- âœ… No button presses required
- âœ… Speech segment capture and queuing
- âœ… Configurable VAD calibration
- âœ… Full PyAudio integration

See [CHANGELOG.md](CHANGELOG.md) for complete version history.

---

## ğŸ¯ Project Status

**Current Version:** 1.1.0  
**Status:** âœ… **Production Ready**  
**Requirements Met:** 98% (all core features complete)

---

## ğŸ“ Support

- Open an issue on GitHub
- Check documentation
- Review troubleshooting section
- See [VOICE_MODE_GUIDE.md](VOICE_MODE_GUIDE.md)

---

**Made with â¤ï¸ for accessible cooking**

*Empowering visually impaired users to cook independently with confidence.*
